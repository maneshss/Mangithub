{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab8be1c-327f-47b1-a81b-74055a767390",
   "metadata": {},
   "source": [
    "## Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7a588-b62d-4d89-baf3-1d9115682879",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "Unemployment is a big socio-economic and political concern for any country and, hence, managing it is a chief task for any government. In this guide, we will try to build regression algorithms for predicting unemployment within an economy.\n",
    "\n",
    "The data used in this project was produced from US economic time series data available from http://research.stlouisfed.org/fred2. The data contains 574 rows and 5 variables, as described below:\n",
    "\n",
    "#### psavert - personal savings rate.\n",
    "#### pce - personal consumption expenditures, in billions of dollars.\n",
    "#### uempmed - median duration of unemployment, in weeks.\n",
    "#### pop - total population, in thousands.\n",
    "#### unemploy- number of unemployed in thousands (dependent variable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31948bbe-4515-4844-8eb9-021fa3d68013",
   "metadata": {},
   "source": [
    "### Step 1 - Loading the Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9d4e982-c233-4760-913d-b6c1d20f1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0a736864-3639-47c6-b74a-fa62ecab8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold,GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8baf75-f505-4f19-9791-2e8b74d28667",
   "metadata": {},
   "source": [
    "### Step 2 - Reading the Data and Performing Basic Data Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "563ed81a-fc91-4de5-af4c-7196a0941357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561, 5)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 561 entries, 0 to 560\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   PCE       561 non-null    int64\n",
      " 1   POP       561 non-null    int64\n",
      " 2   PSAVERT   561 non-null    int64\n",
      " 3   UNEMPLOY  561 non-null    int64\n",
      " 4   UEMPMED   561 non-null    int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 22.0 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCE</th>\n",
       "      <th>POP</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>UNEMPLOY</th>\n",
       "      <th>UEMPMED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7302.024955</td>\n",
       "      <td>278991.654189</td>\n",
       "      <td>7.770053</td>\n",
       "      <td>8356.536542</td>\n",
       "      <td>9.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4529.315730</td>\n",
       "      <td>36192.001529</td>\n",
       "      <td>3.088382</td>\n",
       "      <td>2437.927997</td>\n",
       "      <td>4.129139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1169.000000</td>\n",
       "      <td>218440.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5481.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3294.000000</td>\n",
       "      <td>244610.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6682.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6543.000000</td>\n",
       "      <td>280976.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7784.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10783.000000</td>\n",
       "      <td>312403.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8937.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18266.000000</td>\n",
       "      <td>334944.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>23050.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PCE            POP     PSAVERT      UNEMPLOY     UEMPMED\n",
       "count    561.000000     561.000000  561.000000    561.000000  561.000000\n",
       "mean    7302.024955  278991.654189    7.770053   8356.536542    9.588235\n",
       "std     4529.315730   36192.001529    3.088382   2437.927997    4.129139\n",
       "min     1169.000000  218440.000000    2.000000   5481.000000    2.000000\n",
       "25%     3294.000000  244610.000000    6.000000   6682.000000    7.000000\n",
       "50%     6543.000000  280976.000000    7.000000   7784.000000    8.000000\n",
       "75%    10783.000000  312403.000000    9.000000   8937.000000   10.000000\n",
       "max    18266.000000  334944.000000   34.000000  23050.000000   25.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  # Importing pandas for data manipulation and analysis\n",
    "\n",
    "# Reading data from an Excel file into a DataFrame\n",
    "df = pd.read_excel('data.xlsx')  \n",
    "\n",
    "# Printing the shape of the DataFrame (rows, columns) to understand its size\n",
    "print(df.shape)  \n",
    "\n",
    "# Displaying a summary of the DataFrame, including data types and non-null counts\n",
    "print(df.info())  \n",
    "\n",
    "# Generating descriptive statistics for numerical columns in the DataFrame\n",
    "df.describe()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7320e-c862-483e-963e-26979f204c0a",
   "metadata": {},
   "source": [
    "## Step 3 - Creating Arrays for the Features and the Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d79e279-0386-43fc-833a-b1956f597fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCE</th>\n",
       "      <th>POP</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>UNEMPLOY</th>\n",
       "      <th>UEMPMED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "      <td>561.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.399760</td>\n",
       "      <td>0.832950</td>\n",
       "      <td>0.228531</td>\n",
       "      <td>8356.536542</td>\n",
       "      <td>0.383529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.247964</td>\n",
       "      <td>0.108054</td>\n",
       "      <td>0.090835</td>\n",
       "      <td>2437.927997</td>\n",
       "      <td>0.165166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.063999</td>\n",
       "      <td>0.652169</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>5481.000000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.180335</td>\n",
       "      <td>0.730301</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>6682.000000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.358207</td>\n",
       "      <td>0.838875</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>7784.000000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.590332</td>\n",
       "      <td>0.932702</td>\n",
       "      <td>0.264706</td>\n",
       "      <td>8937.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23050.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PCE         POP     PSAVERT      UNEMPLOY     UEMPMED\n",
       "count  561.000000  561.000000  561.000000    561.000000  561.000000\n",
       "mean     0.399760    0.832950    0.228531   8356.536542    0.383529\n",
       "std      0.247964    0.108054    0.090835   2437.927997    0.165166\n",
       "min      0.063999    0.652169    0.058824   5481.000000    0.080000\n",
       "25%      0.180335    0.730301    0.176471   6682.000000    0.280000\n",
       "50%      0.358207    0.838875    0.205882   7784.000000    0.320000\n",
       "75%      0.590332    0.932702    0.264706   8937.000000    0.400000\n",
       "max      1.000000    1.000000    1.000000  23050.000000    1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the target column (dependent variable) to be used for modeling or analysis\n",
    "target_column = ['UNEMPLOY']\n",
    "\n",
    "# Creating a list of predictor (independent) columns by excluding the target column from all columns in the DataFrame\n",
    "predictors = list(set(list(df.columns)) - set(target_column))\n",
    "\n",
    "# Normalizing the predictor columns by dividing each value by the maximum value in its respective column\n",
    "df[predictors] = df[predictors] / df[predictors].max()\n",
    "\n",
    "# Generating descriptive statistics for the DataFrame after normalization\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4607a3c-efd3-4c67-a373-f5837e7d3c14",
   "metadata": {},
   "source": [
    "## Step 4 - Creating the Training and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ad10ba2-18e2-4a0f-a8b1-a2e97e1ca0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 4)\n",
      "(169, 4)\n"
     ]
    }
   ],
   "source": [
    "# Extracting predictor (independent variable) values as a NumPy array\n",
    "X = df[predictors].values\n",
    "\n",
    "# Extracting target (dependent variable) values as a NumPy array\n",
    "y = df[target_column].values\n",
    "\n",
    "# Splitting the dataset into training and testing sets (70% training, 30% testing)\n",
    "# random_state ensures reproducibility of the split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "\n",
    "# Printing the shape of the training and testing sets for predictors (X)\n",
    "print(X_train.shape)  # Shape of the training set (rows, columns)\n",
    "print(X_test.shape)   # Shape of the testing set (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50dcee-b90c-49e2-9e44-f40d8b3326bb",
   "metadata": {},
   "source": [
    "## Step 5 - Build, Predict and Evaluate the Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c36f4ad3-9449-47a8-915a-1baa8027a84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Training (fitting) the Linear Regression model using the training data\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "512c4531-73cc-46b4-be66-1cde96256d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497.478192850254\n",
      "0.6137780635501369\n",
      "1380.0909818486987\n",
      "0.693523637493907\n"
     ]
    }
   ],
   "source": [
    "# Predicting target values for the training data using the trained Linear Regression model\n",
    "pred_train_lr = lr.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_lr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_lr))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Linear Regression model\n",
    "pred_test_lr = lr.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_lr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988cdcf8-5094-4357-aaa5-ba819a4ab744",
   "metadata": {},
   "source": [
    "## Regularized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e14bf-1e71-4954-a9c3-f38b6add9654",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb888e83-0a5e-4523-895b-17005a5945da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1498.093457657057\n",
      "0.6134606264305966\n",
      "1385.726009342799\n",
      "0.6910157905469532\n"
     ]
    }
   ],
   "source": [
    "# Initializing a Ridge Regression model with a regularization strength (alpha)\n",
    "# Alpha is the hyperparameter that controls the degree of regularization:\n",
    "# - A smaller alpha (e.g., 0.01) applies less regularization and behaves closer to standard linear regression.\n",
    "# - A larger alpha increases regularization, reducing the model's complexity to prevent overfitting.\n",
    "rr = Ridge(alpha=0.01)\n",
    "\n",
    "# Training (fitting) the Ridge Regression model using the training data\n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Ridge Regression model\n",
    "pred_train_rr = rr.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_rr))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Ridge Regression model\n",
    "pred_test_rr = rr.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21168f73-8baf-4a69-9adb-cbf4625de036",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The above output shows that the RMSE and R-squared values for the Ridge Regression model on the training data is 1498 thousand and 61.3 percent, respectively. For the test data, the result for these metrics is 1895 thousand and 69 percent, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c123401-97b6-4929-b5d6-f42e7b5ed16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497.4782632414215\n",
      "0.6137780272402742\n",
      "1380.142892758321\n",
      "0.693500581382136\n"
     ]
    }
   ],
   "source": [
    "# Setting alpha as 0.0001, which is very close to 0\n",
    "# - A smaller alpha value applies minimal regularization, making Ridge Regression behave similarly to standard Linear Regression.\n",
    "# - This can result in a better fit for training data but may increase the risk of overfitting for test data.\n",
    "rr = Ridge(alpha=0.0001)\n",
    "\n",
    "# Training (fitting) the Ridge Regression model using the training data\n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Ridge Regression model\n",
    "pred_train_rr = rr.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_rr))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Ridge Regression model\n",
    "pred_test_rr = rr.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b9d75ea-daf6-4513-bf03-5a6cfaddca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501.8007228969986\n",
      "0.6115451557077576\n",
      "1398.4722561111562\n",
      "0.6853054234367043\n"
     ]
    }
   ],
   "source": [
    "# Setting alpha as 0.03 for moderate regularization\n",
    "# - A slightly higher alpha compared to very small values (e.g., 0.0001) introduces more regularization.\n",
    "# - This helps in balancing the trade-off between overfitting (too complex) and underfitting (too simple).\n",
    "rr = Ridge(alpha=0.03)\n",
    "\n",
    "# Training (fitting) the Ridge Regression model using the training data\n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Ridge Regression model\n",
    "pred_train_rr = rr.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_rr))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Ridge Regression model\n",
    "pred_test_rr = rr.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3f1084c-41cb-4081-84bc-5cb7347723d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499.6444649765567\n",
      "0.6126598275950197\n",
      "1391.9608149346202\n",
      "0.6882291064223477\n"
     ]
    }
   ],
   "source": [
    "# Setting alpha as 0.02 for moderate regularization\n",
    "# - A slightly higher alpha compared to very small values like 0.0001 still applies regularization.\n",
    "# - This helps in balancing the trade-off between overfitting and underfitting.\n",
    "rr = Ridge(alpha=0.02)\n",
    "\n",
    "# Training (fitting) the Ridge Regression model using the training data\n",
    "rr.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Ridge Regression model\n",
    "pred_train_rr = rr.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_rr))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Ridge Regression model\n",
    "pred_test_rr = rr.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_rr)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_rr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d30e8-39c6-4e4e-abca-d9b0760c3ab9",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n",
    "\n",
    "The loss function for Lasso Regression can be expressed as below:\n",
    "\n",
    "Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "71018cb2-941b-417e-9965-430d69e32c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497.478370670584\n",
      "0.6137779718251066\n",
      "1380.177110161681\n",
      "0.6934853833264865\n"
     ]
    }
   ],
   "source": [
    "# Initializing a Lasso Regression model with a regularization parameter (alpha)\n",
    "# - Lasso (Least Absolute Shrinkage and Selection Operator) applies L1 regularization.\n",
    "# - The alpha parameter controls the strength of the regularization:\n",
    "#   - Smaller alpha values apply less regularization, allowing more coefficients to remain non-zero.\n",
    "#   - Larger alpha values apply stronger regularization, potentially shrinking more coefficients to zero (feature selection).\n",
    "model_lasso = Lasso(alpha=0.01)\n",
    "\n",
    "# Training (fitting) the Lasso Regression model using the training data\n",
    "model_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Lasso Regression model\n",
    "pred_train_lasso = model_lasso.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_lasso)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_lasso))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Lasso Regression model\n",
    "pred_test_lasso = model_lasso.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_lasso)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef3ca6c5-3151-462f-a362-3025ed156088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1497.9206123344165\n",
      "0.6135498166852426\n",
      "1384.9523443675569\n",
      "0.6913607123093292\n"
     ]
    }
   ],
   "source": [
    "# Initializing a Lasso Regression model with a higher regularization parameter (alpha=0.5)\n",
    "# - A larger alpha applies stronger L1 regularization, which:\n",
    "#   - Shrinks coefficients more aggressively.\n",
    "#   - Forces some coefficients to become exactly zero, effectively performing feature selection.\n",
    "model_lasso1 = Lasso(alpha=0.5)\n",
    "\n",
    "# Training (fitting) the Lasso Regression model using the training data\n",
    "model_lasso1.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Lasso Regression model\n",
    "pred_train_lasso1 = model_lasso1.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_lasso1)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_lasso1))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Lasso Regression model\n",
    "pred_test_lasso1 = model_lasso1.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_lasso1)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_lasso1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5199483-f843-49af-b059-852d574b33b3",
   "metadata": {},
   "source": [
    "### CV-Hyper parameter tunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7207665-acef-40c2-8732-f3796e890699",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 561745629.6253227, tolerance: 306403.84993346536\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548430921.2096065, tolerance: 290866.3941839286\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 532121849.54951274, tolerance: 289551.41113742575\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520957823.6978905, tolerance: 299332.1648407921\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 508718847.7801591, tolerance: 293674.3674110891\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498237317.3122758, tolerance: 302711.2775437624\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 550843758.4620764, tolerance: 294070.58178336633\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518492304.3742691, tolerance: 301240.9781968254\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 517483575.60917336, tolerance: 309220.1516661386\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526550191.21505857, tolerance: 298774.9224633664\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 536744989.5965269, tolerance: 304391.7288760396\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525487244.6356934, tolerance: 281930.4841326733\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 566852451.4532932, tolerance: 318619.6995714851\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 570490027.0682395, tolerance: 302666.9622843565\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555034384.4569764, tolerance: 300013.52656646824\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 560768440.4368165, tolerance: 306111.9801940594\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 546490601.0979533, tolerance: 295900.1808843565\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549391691.7427715, tolerance: 305734.0158126733\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 553312605.743609, tolerance: 305626.31238613866\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 514279397.9098878, tolerance: 303716.31486653467\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 535199755.90318567, tolerance: 290825.4412047525\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 563000886.8630196, tolerance: 307069.41938891093\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 529782693.9913157, tolerance: 285105.8830443565\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527122471.2184517, tolerance: 292168.00927920797\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 538951441.9507298, tolerance: 298673.3452368317\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557522357.0103562, tolerance: 303634.22288712877\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 539074616.3071638, tolerance: 308756.5277017822\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 561840991.443685, tolerance: 313824.0793552475\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512182063.9453775, tolerance: 292999.55831405945\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:598: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500110696.3244199, tolerance: 281136.23977346544\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -1070.68260\n",
      "Config: {'alpha': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Setting up cross-validation with RepeatedKFold\n",
    "# - n_splits=10: Divides the data into 10 folds for cross-validation.\n",
    "# - n_repeats=3: Repeats the cross-validation process 3 times for more robust results.\n",
    "# - random_state=1: Ensures reproducibility of the folds.\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Generating a range of alpha values from 0 to 0.2 with 21 equally spaced points\n",
    "# - These alpha values will be tested to find the optimal regularization strength for Lasso.\n",
    "lasso_alphas = np.linspace(0, 0.2, 21)\n",
    "\n",
    "# Initializing a Lasso Regression model (without setting alpha explicitly yet)\n",
    "lasso = Lasso()\n",
    "\n",
    "# Defining a dictionary for the hyperparameter grid\n",
    "# - The key 'alpha' corresponds to the list of alpha values to be tested.\n",
    "grid = dict()\n",
    "grid['alpha'] = lasso_alphas\n",
    "\n",
    "# Setting up a GridSearchCV to find the best alpha value\n",
    "# - `lasso`: The Lasso Regression model.\n",
    "# - `grid`: The hyperparameter grid containing alpha values.\n",
    "# - `scoring='neg_mean_absolute_error'`: The performance metric to minimize.\n",
    "# - `cv=cv`: The cross-validation strategy defined earlier.\n",
    "# - `n_jobs=-1`: Utilizes all available CPU cores for parallel processing.\n",
    "gscv = GridSearchCV(\n",
    "    lasso, grid, scoring='neg_mean_absolute_error',\n",
    "    cv=cv, n_jobs=-1\n",
    ")\n",
    "\n",
    "# Performing the grid search to find the best alpha value\n",
    "results = gscv.fit(X, y)\n",
    "\n",
    "# Printing the best cross-validated mean absolute error (MAE) score\n",
    "print('MAE: %.5f' % results.best_score_)\n",
    "\n",
    "# Printing the optimal alpha value found during grid search\n",
    "print('Config: %s' % results.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1cb7bb-a6aa-4c65-9218-bb24f396a97d",
   "metadata": {},
   "source": [
    "### ElasticNet Regression\n",
    "ElasticNet combines the properties of both Ridge and Lasso regression. It works by penalizing the model using both the l2-norm and the l1-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b821ef3-18a1-4b0f-b7ac-eef3bd379c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1664.4184641637066\n",
      "0.5228653312951645\n",
      "1671.188594639023\n",
      "0.5506006141602047\n"
     ]
    }
   ],
   "source": [
    "# Elastic Net Regression model with L1 and L2 regularization\n",
    "# - `alpha=0.01`: Controls the overall strength of regularization.\n",
    "# - Elastic Net combines L1 (Lasso) and L2 (Ridge) penalties for regularization.\n",
    "model_enet = ElasticNet(alpha=0.01)\n",
    "\n",
    "# Training (fitting) the Elastic Net model using the training data\n",
    "model_enet.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values for the training data using the trained Elastic Net model\n",
    "pred_train_enet = model_enet.predict(X_train)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the training predictions\n",
    "print(np.sqrt(mean_squared_error(y_train, pred_train_enet)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the training predictions (model's performance on training data)\n",
    "print(r2_score(y_train, pred_train_enet))\n",
    "\n",
    "# Predicting target values for the testing data using the trained Elastic Net model\n",
    "pred_test_enet = model_enet.predict(X_test)\n",
    "\n",
    "# Calculating and printing the root mean squared error (RMSE) for the testing predictions\n",
    "print(np.sqrt(mean_squared_error(y_test, pred_test_enet)))\n",
    "\n",
    "# Calculating and printing the R-squared score for the testing predictions (model's performance on testing data)\n",
    "print(r2_score(y_test, pred_test_enet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63df9b40-6526-4437-9713-885947af9213",
   "metadata": {},
   "source": [
    "The ElasticNet Regression model is performing the worst. All the other regression models are performing better with a decent R-squared and stable RMSE values. The most ideal result would be an RMSE value of zero and R-squared value of 1, but that's almost impossible in real economic datasets.\n",
    "\n",
    "There are other iterations that can be done to improve model performance. We have assigned the value of alpha to be 0.01, but this can be altered by hyper parameter tuning to arrive at the optimal alpha value. Cross-validation can also be tried along with feature selection techniques. However, that is not covered in this guide which was aimed at enabling individuals to understand and implement the various Linear Regression models using the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f357f34-18ad-4087-ace0-72a15ab1966a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37532808, 0.66029172, 0.49923516, 0.64016747, 0.59784285])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Linear Regression model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Performing k-fold cross-validation (5 folds in this case)\n",
    "# - `lm`: The Linear Regression model to evaluate.\n",
    "# - `X_train, y_train`: The training dataset used for cross-validation.\n",
    "# - `scoring='r2'`: R-squared is used as the evaluation metric to measure model performance.\n",
    "# - `cv=5`: Specifies 5-fold cross-validation, meaning the training set is divided into 5 parts:\n",
    "#   - 4 parts are used for training, and 1 part is used for validation.\n",
    "#   - This process is repeated 5 times, with each part serving as the validation set once.\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=5)\n",
    "\n",
    "# Displaying the R-squared scores for each fold\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67289e4a-d30a-44d1-9145-32625c53d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3154300.02472881, -2324246.90997098, -3041345.78502868,\n",
       "       -2328392.41592552, -1741550.35886557])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a Linear Regression model\n",
    "lm1 = LinearRegression()\n",
    "\n",
    "# Performing k-fold cross-validation (5 folds in this case)\n",
    "# - `lm1`: The Linear Regression model to evaluate.\n",
    "# - `X_train, y_train`: The training dataset used for cross-validation.\n",
    "# - `scoring='neg_mean_squared_error'`: The evaluation metric is negative mean squared error (MSE).\n",
    "#   - Negative MSE is used because cross_val_score expects higher values to indicate better performance.\n",
    "#   - To interpret the results, take the absolute value of the scores or multiply by -1.\n",
    "# - `cv=5`: Specifies 5-fold cross-validation (training on 4 folds and validating on the remaining fold, repeated 5 times).\n",
    "scores = cross_val_score(lm1, X_train, y_train, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Displaying the negative MSE scores for each fold\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "423bf08e-2ade-46b4-9ae3-9cb55669ad84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the sklearn library to access its metrics module\n",
    "import sklearn\n",
    "\n",
    "# Retrieving and displaying a sorted list of all available scoring metrics in sklearn\n",
    "# - `sklearn.metrics.SCORERS.keys()`: Returns a dictionary of scoring metrics that can be used in functions like `cross_val_score` or `GridSearchCV`.\n",
    "# - `sorted()`: Sorts the list of metric names for easier readability.\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40c713f1-594e-4d0b-84c4-046eab175dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=100, shuffle=True),\n",
       "             estimator=RFE(estimator=LinearRegression()),\n",
       "             param_grid=[{'n_features_to_select': [1, 2, 3]}],\n",
       "             return_train_score=True, scoring='r2', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Create a cross-validation scheme\n",
    "# - `KFold`: Splits the data into `n_splits` folds for cross-validation.\n",
    "# - `shuffle=True`: Shuffles the data before splitting to ensure randomness.\n",
    "# - `random_state=100`: Ensures reproducibility of the splits.\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=100)\n",
    "\n",
    "# Step 2: Specify the range of hyperparameters to tune\n",
    "# - `n_features_to_select`: List of the number of features to be selected by Recursive Feature Elimination (RFE).\n",
    "# - Here, the range is set to 1 to 3, meaning RFE will try selecting 1, 2, or 3 features.\n",
    "hyper_params = [{'n_features_to_select': list(range(1, 4))}]\n",
    "\n",
    "# Step 3: Perform grid search\n",
    "# 3.1 Specify the base model\n",
    "# - `lm`: A Linear Regression model is initialized as the base model for RFE.\n",
    "lm = LinearRegression()\n",
    "\n",
    "# - `fit`: Fits the Linear Regression model on the training data.\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# - `RFE`: Recursive Feature Elimination is initialized with the base model (`lm`).\n",
    "#   RFE will rank and recursively eliminate features based on their importance.\n",
    "rfe = RFE(lm)\n",
    "\n",
    "# 3.2 Initialize GridSearchCV\n",
    "# - `estimator=rfe`: Specifies RFE as the model for hyperparameter tuning.\n",
    "# - `param_grid=hyper_params`: The grid of hyperparameters to test (number of features to select).\n",
    "# - `scoring='r2'`: Uses R-squared as the evaluation metric for model performance.\n",
    "# - `cv=folds`: Applies the KFold cross-validation scheme.\n",
    "# - `verbose=1`: Displays progress logs during grid search.\n",
    "# - `return_train_score=True`: Stores training scores for each fold.\n",
    "model_cv = GridSearchCV(\n",
    "    estimator=rfe, \n",
    "    param_grid=hyper_params, \n",
    "    scoring='r2', \n",
    "    cv=folds, \n",
    "    verbose=1, \n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV model on the training data\n",
    "model_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4a2c6d1-7275-4473-88c6-fbe955aad848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_features_to_select</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010243</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_features_to_select': 1}</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.382173</td>\n",
       "      <td>-0.027519</td>\n",
       "      <td>0.365837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141274</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.032341</td>\n",
       "      <td>0.430245</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>0.434023</td>\n",
       "      <td>0.047866</td>\n",
       "      <td>0.198567</td>\n",
       "      <td>0.190797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005261</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_features_to_select': 2}</td>\n",
       "      <td>0.118136</td>\n",
       "      <td>0.357447</td>\n",
       "      <td>0.100838</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195754</td>\n",
       "      <td>0.145798</td>\n",
       "      <td>2</td>\n",
       "      <td>0.096235</td>\n",
       "      <td>0.499913</td>\n",
       "      <td>0.097406</td>\n",
       "      <td>0.492705</td>\n",
       "      <td>0.111870</td>\n",
       "      <td>0.259626</td>\n",
       "      <td>0.193343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_features_to_select': 3}</td>\n",
       "      <td>0.260561</td>\n",
       "      <td>0.384150</td>\n",
       "      <td>0.207754</td>\n",
       "      <td>0.411395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291270</td>\n",
       "      <td>0.090258</td>\n",
       "      <td>1</td>\n",
       "      <td>0.338543</td>\n",
       "      <td>0.542744</td>\n",
       "      <td>0.353793</td>\n",
       "      <td>0.534538</td>\n",
       "      <td>0.347334</td>\n",
       "      <td>0.423390</td>\n",
       "      <td>0.094262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.010243      0.010287         0.000727        0.000901   \n",
       "1       0.005261      0.003447         0.000361        0.000175   \n",
       "2       0.001317      0.000200         0.000300        0.000075   \n",
       "\n",
       "  param_n_features_to_select                       params  split0_test_score  \\\n",
       "0                          1  {'n_features_to_select': 1}           0.044350   \n",
       "1                          2  {'n_features_to_select': 2}           0.118136   \n",
       "2                          3  {'n_features_to_select': 3}           0.260561   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.382173          -0.027519           0.365837  ...   \n",
       "1           0.357447           0.100838           0.381519  ...   \n",
       "2           0.384150           0.207754           0.411395  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.141274        0.193000                3            0.032341   \n",
       "1         0.195754        0.145798                2            0.096235   \n",
       "2         0.291270        0.090258                1            0.338543   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.430245            0.048360            0.434023   \n",
       "1            0.499913            0.097406            0.492705   \n",
       "2            0.542744            0.353793            0.534538   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.047866          0.198567         0.190797  \n",
       "1            0.111870          0.259626         0.193343  \n",
       "2            0.347334          0.423390         0.094262  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv results/Output\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb7c9f5-4983-49d2-af75-a68e60a536ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
